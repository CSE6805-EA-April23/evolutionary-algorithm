{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt-8eVeu2026",
        "outputId": "52acc3dd-d96c-4ba3-d1e4-3d79ac397ec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnnyNrHuywgq",
        "outputId": "657c5f2f-6ae0-402e-b0c8-1670a2a005af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGU7-d-J52Ux",
        "outputId": "338c4d42-ed31-4233-99a2-f3b597ae86e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: nureanikaanan\n",
            "Your Kaggle Key: ··········\n",
            "Downloading titanic.zip to ./titanic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34.1k/34.1k [00:00<00:00, 33.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting archive ./titanic/titanic.zip to ./titanic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "import pandas\n",
        "\n",
        "od.download(\"https://www.kaggle.com/competitions/titanic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IZAMU2fU6ipI"
      },
      "outputs": [],
      "source": [
        "# !pip install numpy==1.24.4\n",
        "# !pip install pandas --upgrade\n",
        "# !pip install matplotlib --upgrade\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn --upgrade\n",
        "!pip install sklearn --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1axHCv4t7uwc",
        "outputId": "088c2d08-a649-4ccd-b088-1d7f9150f05b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.0-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.16.0)\n",
            "Installing collected packages: seaborn\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.12.2\n",
            "    Uninstalling seaborn-0.12.2:\n",
            "      Successfully uninstalled seaborn-0.12.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed seaborn-0.13.0\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post10.tar.gz (3.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1tPjiFCO6WmA"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oIXDK5nw8thp"
      },
      "outputs": [],
      "source": [
        "# load dataset using pandas\n",
        "titanic_train_df = pd.read_csv('./titanic/train.csv')\n",
        "titanic_test_df = pd.read_csv('./titanic/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2VDxi6DL9Ipn"
      },
      "outputs": [],
      "source": [
        "titanic_Y_col = titanic_train_df.columns[1]\n",
        "titanic_X_col = titanic_train_df.columns[2:]\n",
        "titanic_X_col = titanic_X_col.drop(['Name','Ticket'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DyPiK5Cn9Zqn"
      },
      "outputs": [],
      "source": [
        "titanic_X, titanic_Y = titanic_train_df[titanic_X_col].copy(), titanic_train_df[titanic_Y_col].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tJkd_rj99lsX"
      },
      "outputs": [],
      "source": [
        "numeric_cols = titanic_train_df[titanic_X_col].select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = titanic_train_df[titanic_X_col].select_dtypes(exclude=np.number).columns.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "U_1TeKN79ryv"
      },
      "outputs": [],
      "source": [
        "# Impute and scale numeric columns\n",
        "imputer = SimpleImputer().fit(titanic_train_df[numeric_cols])\n",
        "titanic_X[numeric_cols] = imputer.transform(titanic_X[numeric_cols])\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler().fit(titanic_X[numeric_cols])\n",
        "titanic_X[numeric_cols] = scaler.transform(titanic_X[numeric_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5-fQpSv9yLI",
        "outputId": "5922aa76-7fa8-43ac-e356-e3f598cb0f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n",
            "<ipython-input-12-e4e5141b2300>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])\n"
          ]
        }
      ],
      "source": [
        "# One-hot encode categorical columns\n",
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(titanic_X[categorical_cols])\n",
        "encoded_cols = list(encoder.get_feature_names_out(categorical_cols))\n",
        "titanic_X[encoded_cols] = encoder.transform(titanic_X[categorical_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "661JrU5T-owI"
      },
      "outputs": [],
      "source": [
        "titanic_X = titanic_X[numeric_cols + encoded_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "B9NEtRtI-t-A"
      },
      "outputs": [],
      "source": [
        "titanic_X_Train, titanic_X_Test, titanic_Y_Train, titanic_Y_Test = train_test_split(titanic_X, titanic_Y, test_size = 0.30,random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "p8pIPoNK-5dQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc2400a-7a33-492f-8042-fafd2e82df5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymoo\n",
            "  Downloading pymoo-0.6.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from pymoo) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pymoo) (1.11.3)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from pymoo) (3.7.1)\n",
            "Requirement already satisfied: autograd>=1.4 in /usr/local/lib/python3.10/dist-packages (from pymoo) (1.6.2)\n",
            "Collecting cma==3.2.2 (from pymoo)\n",
            "  Downloading cma-3.2.2-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alive-progress (from pymoo)\n",
            "  Downloading alive_progress-3.1.4-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from pymoo)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated (from pymoo)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.4->pymoo) (0.18.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo) (2.8.2)\n",
            "Collecting about-time==4.2.1 (from alive-progress->pymoo)\n",
            "  Downloading about_time-4.2.1-py3-none-any.whl (13 kB)\n",
            "Collecting grapheme==0.6.0 (from alive-progress->pymoo)\n",
            "  Downloading grapheme-0.6.0.tar.gz (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->pymoo) (1.14.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3->pymoo) (1.16.0)\n",
            "Building wheels for collected packages: grapheme\n",
            "  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grapheme: filename=grapheme-0.6.0-py3-none-any.whl size=210079 sha256=edb34c09e396239dd335d45bddcade09774da7d0adbce06a565e8eae8c199cef\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/e1/49/37e6bde9886439057450c494a79b0bef8bbe897a54aebfc757\n",
            "Successfully built grapheme\n",
            "Installing collected packages: grapheme, dill, Deprecated, cma, about-time, alive-progress, pymoo\n",
            "Successfully installed Deprecated-1.2.14 about-time-4.2.1 alive-progress-3.1.4 cma-3.2.2 dill-0.3.7 grapheme-0.6.0 pymoo-0.6.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pymoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jaFweg5d_Nai"
      },
      "outputs": [],
      "source": [
        "from pymoo.algorithms.moo.nsga3 import NSGA3\n",
        "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
        "from pymoo.factory import get_problem, get_reference_directions, get_sampling, get_crossover, get_mutation, get_termination\n",
        "from pymoo.operators.selection.rnd import RandomSelection\n",
        "from pymoo.operators.crossover.sbx import SBX\n",
        "from pymoo.operators.mutation.pm import PolynomialMutation\n",
        "from pymoo.termination.default import DefaultMultiObjectiveTermination\n",
        "from pymoo.core.problem import Problem\n",
        "from pymoo.optimize import minimize\n",
        "from pymoo.operators.sampling.lhs import LHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JNElZIJdAwqk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "J3Tw5cORA7yx"
      },
      "outputs": [],
      "source": [
        "def test_params(**params):\n",
        "    model = RandomForestClassifier(random_state=42, n_jobs=-1, **params).fit(titanic_X_Train, titanic_Y_Train)\n",
        "    train_accuracy_score = accuracy_score(titanic_Y_Train, model.predict(titanic_X_Train))\n",
        "    val_accuracy_score = accuracy_score(titanic_Y_Test, model.predict(titanic_X_Test))\n",
        "    return train_accuracy_score, val_accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "u_sqyoAA_Y1Z"
      },
      "outputs": [],
      "source": [
        "# define the hyperparameter optimization problem\n",
        "class HyperparameterOptimizationProblem(Problem):\n",
        "\n",
        "    def __init__(self,level):\n",
        "        # define the lower and upper bounds of the hyperparameters\n",
        "        # n_estimators: number of trees in the forest (integer)\n",
        "        # max_depth: maximum depth of each tree (integer)\n",
        "        # max_features: maximum number of features (integer)\n",
        "        # min_samples_leaf: minimum number of samples required to be at a leaf node (integer)\n",
        "        xl = np.array([1, 2, 2, 1, 0])\n",
        "        xu = np.array([600, 12, 40, 9, 0.5])\n",
        "        self.level=level\n",
        "\n",
        "        # initialize the problem with 4 variables and 2 objectives\n",
        "        super().__init__(n_var = 5, n_obj = 2, xl = xl, xu = xu)\n",
        "\n",
        "    def _evaluate(self, x, out, *args, **kwargs):\n",
        "        # evaluate each solution (each row of x)\n",
        "        f = np.zeros((x.shape[0], self.n_obj))\n",
        "        for i in range(x.shape[0]):\n",
        "          if self.level==0:\n",
        "            # get the hyperparameters\n",
        "            n_estimators = int(x[i, 0])*3\n",
        "            max_depth = int(x[i, 1])\n",
        "            min_samples_split = int(x[i, 2])*2\n",
        "            min_samples_leaf = int(x[i, 3])\n",
        "            min_weight_fraction_leaf = int(x[i, 4])\n",
        "          else:\n",
        "            # get the hyperparameters\n",
        "            n_estimators = int(x[i, 0])\n",
        "            max_depth = int(x[i, 1])\n",
        "            min_samples_split = int(x[i, 2])\n",
        "            min_samples_leaf = int(x[i, 3])\n",
        "            min_weight_fraction_leaf = int(x[i, 4])\n",
        "\n",
        "\n",
        "            # build and train the random forest model\n",
        "            model = RandomForestClassifier(n_estimators=n_estimators,\n",
        "                                           max_depth=max_depth,\n",
        "                                           min_samples_split=min_samples_split,\n",
        "                                           min_samples_leaf = min_samples_leaf,\n",
        "                                           min_weight_fraction_leaf = min_weight_fraction_leaf,\n",
        "                                           max_features=\"sqrt\",\n",
        "                                           random_state=42,\n",
        "                                           n_jobs = -1)\n",
        "            model.fit(titanic_X_Train, titanic_Y_Train)\n",
        "\n",
        "            # predict on the test set\n",
        "            y_pred = model.predict(titanic_X_Test)\n",
        "\n",
        "            # calculate the accuracy, f1 and ROC/AUC score as the objectives\n",
        "            f[i, 0] = -accuracy_score(titanic_Y_Test, y_pred) # negate because we want to maximize\n",
        "            f[i, 1] = -f1_score(titanic_Y_Test, y_pred) # negate because we want to maximize\n",
        "            # f[i, 2] = -roc_auc_score(titanic_Y_Test, y_pred) # negate because we want to maximize\n",
        "\n",
        "\n",
        "        # assign the objectives to the output dictionary\n",
        "        out[\"F\"] = f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2UtYt5uI_3aE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc2220d-356a-4119-da42-89624d1be4a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================\n",
            "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
            "==========================================================\n",
            "     1 |      100 |    100 |             - |             -\n",
            "     2 |      200 |    100 |  0.000000E+00 |             f\n",
            "     3 |      300 |    100 |  0.000000E+00 |             f\n",
            "     4 |      400 |    100 |  0.000000E+00 |             f\n",
            "     5 |      500 |    100 |  0.000000E+00 |             f\n",
            "     6 |      600 |    100 |  0.000000E+00 |             f\n",
            "     7 |      700 |    100 |  0.000000E+00 |             f\n",
            "     8 |      800 |    100 |  0.000000E+00 |             f\n",
            "     9 |      900 |    100 |  0.000000E+00 |             f\n",
            "    10 |     1000 |    100 |  0.000000E+00 |             f\n",
            "    11 |     1100 |    100 |  0.000000E+00 |             f\n",
            "    12 |     1200 |    100 |  0.000000E+00 |             f\n",
            "    13 |     1300 |    100 |  0.000000E+00 |             f\n",
            "    14 |     1400 |    100 |  0.000000E+00 |             f\n",
            "    15 |     1500 |    100 |  0.000000E+00 |             f\n",
            "    16 |     1600 |    100 |  0.000000E+00 |             f\n",
            "    17 |     1700 |    100 |  0.000000E+00 |             f\n",
            "    18 |     1800 |    100 |  0.000000E+00 |             f\n",
            "    19 |     1900 |    100 |  0.000000E+00 |             f\n",
            "    20 |     2000 |    100 |  0.000000E+00 |             f\n",
            "    21 |     2100 |    100 |  0.000000E+00 |             f\n",
            "    22 |     2200 |    100 |  0.000000E+00 |             f\n",
            "    23 |     2300 |    100 |  0.000000E+00 |             f\n",
            "    24 |     2400 |    100 |  0.000000E+00 |             f\n",
            "    25 |     2500 |    100 |  0.000000E+00 |             f\n",
            "    26 |     2600 |    100 |  0.000000E+00 |             f\n",
            "    27 |     2700 |    100 |  0.000000E+00 |             f\n",
            "    28 |     2800 |    100 |  0.000000E+00 |             f\n",
            "    29 |     2900 |    100 |  0.000000E+00 |             f\n",
            "    30 |     3000 |    100 |  0.000000E+00 |             f\n",
            "    31 |     3100 |    100 |  0.000000E+00 |             f\n",
            "    32 |     3200 |    100 |  0.000000E+00 |             f\n",
            "    33 |     3300 |    100 |  0.000000E+00 |             f\n",
            "    34 |     3400 |    100 |  0.000000E+00 |             f\n",
            "    35 |     3500 |    100 |  0.000000E+00 |             f\n",
            "    36 |     3600 |    100 |  0.000000E+00 |             f\n",
            "==========================================================\n",
            "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
            "==========================================================\n",
            "     1 |       50 |      1 |             - |             -\n",
            "     2 |      100 |      1 |  0.000000E+00 |             f\n",
            "     3 |      150 |      2 |  1.0000000000 |         ideal\n",
            "     4 |      200 |      5 |  0.000000E+00 |             f\n",
            "     5 |      250 |      6 |  0.000000E+00 |             f\n",
            "     6 |      300 |      5 |  6.4777486911 |         nadir\n",
            "     7 |      350 |      1 |  0.0061454421 |         ideal\n",
            "     8 |      400 |      1 |  0.000000E+00 |             f\n",
            "     9 |      450 |      2 |  0.000000E+00 |             f\n",
            "    10 |      500 |      3 |  0.000000E+00 |             f\n",
            "    11 |      550 |      1 |  0.0037313433 |         ideal\n",
            "    12 |      600 |      2 |  0.000000E+00 |             f\n",
            "    13 |      650 |      2 |  0.000000E+00 |             f\n",
            "    14 |      700 |      4 |  0.000000E+00 |             f\n",
            "    15 |      750 |      7 |  0.000000E+00 |             f\n",
            "    16 |      800 |     14 |  0.000000E+00 |             f\n",
            "    17 |      850 |      2 |  0.0061691542 |         ideal\n",
            "    18 |      900 |      6 |  0.000000E+00 |             f\n",
            "    19 |      950 |     13 |  0.000000E+00 |             f\n",
            "    20 |     1000 |     23 |  0.000000E+00 |             f\n",
            "    21 |     1050 |     42 |  0.000000E+00 |             f\n",
            "    22 |     1100 |     50 |  0.000000E+00 |             f\n",
            "    23 |     1150 |     50 |  0.000000E+00 |             f\n",
            "    24 |     1200 |     50 |  0.000000E+00 |             f\n",
            "    25 |     1250 |     50 |  0.000000E+00 |             f\n",
            "    26 |     1300 |     50 |  0.000000E+00 |             f\n",
            "    27 |     1350 |     50 |  0.000000E+00 |             f\n",
            "    28 |     1400 |     50 |  0.000000E+00 |             f\n",
            "    29 |     1450 |     50 |  0.000000E+00 |             f\n",
            "    30 |     1500 |     50 |  0.000000E+00 |             f\n",
            "    31 |     1550 |     50 |  0.000000E+00 |             f\n",
            "    32 |     1600 |     50 |  0.000000E+00 |             f\n",
            "    33 |     1650 |     50 |  0.000000E+00 |             f\n",
            "    34 |     1700 |     50 |  0.000000E+00 |             f\n",
            "    35 |     1750 |     50 |  0.000000E+00 |             f\n",
            "    36 |     1800 |     50 |  0.000000E+00 |             f\n",
            "    37 |     1850 |     50 |  0.000000E+00 |             f\n",
            "    38 |     1900 |     50 |  0.000000E+00 |             f\n",
            "    39 |     1950 |     50 |  0.000000E+00 |             f\n",
            "    40 |     2000 |     50 |  0.000000E+00 |             f\n",
            "    41 |     2050 |     50 |  0.000000E+00 |             f\n",
            "    42 |     2100 |     50 |  0.000000E+00 |             f\n",
            "    43 |     2150 |     50 |  0.000000E+00 |             f\n",
            "    44 |     2200 |     50 |  0.000000E+00 |             f\n",
            "    45 |     2250 |     50 |  0.000000E+00 |             f\n",
            "    46 |     2300 |     50 |  0.000000E+00 |             f\n",
            "    47 |     2350 |     50 |  0.000000E+00 |             f\n",
            "    48 |     2400 |     50 |  0.000000E+00 |             f\n",
            "    49 |     2450 |     50 |  0.000000E+00 |             f\n",
            "    50 |     2500 |     50 |  0.000000E+00 |             f\n",
            "Best solution found: \n",
            "X = [[15  4 34  6  0]\n",
            " [15  4 35  6  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 39  6  0]\n",
            " [15  4 36  5  0]\n",
            " [15  4 33  5  0]\n",
            " [15  4 34  6  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  6  0]\n",
            " [15  4 33  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 34  5  0]\n",
            " [15  4 38  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 33  5  0]\n",
            " [15  4 35  6  0]\n",
            " [15  4 34  6  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 34  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 37  6  0]\n",
            " [15  4 35  6  0]\n",
            " [15  4 34  6  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 34  6  0]\n",
            " [15  4 33  5  0]\n",
            " [15  4 36  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 36  6  0]\n",
            " [15  4 33  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  6  0]\n",
            " [15  4 36  5  0]\n",
            " [15  4 34  5  0]\n",
            " [15  4 33  5  0]\n",
            " [15  4 33  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 36  5  0]\n",
            " [15  4 34  6  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 37  6  0]\n",
            " [15  4 35  5  0]] \n",
            "F = [[0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]]\n",
            "CPU times: user 7min 46s, sys: 34.5 s, total: 8min 21s\n",
            "Wall time: 8min 42s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# create an instance of the problem\n",
        "problem = HyperparameterOptimizationProblem(level=0)\n",
        "problem1 = HyperparameterOptimizationProblem(level=1)\n",
        "\n",
        "# create an instance of NSGA-III algorithm\n",
        "algorithm = NSGA2(\n",
        "    pop_size= 100,\n",
        "    #ref_dirs=get_reference_directions(\"das-dennis\", 3, n_partitions=12),\n",
        "    # sampling=get_sampling(\"int_random\"),\n",
        "    sampling=LHS(),\n",
        "    selection = RandomSelection(),\n",
        "    # crossover=get_crossover(\"int_sbx\", prob=0.9, eta=15),\n",
        "    crossover = SBX(prob=0.9, prob_var=0.8),\n",
        "    mutation=PolynomialMutation(prob=0.8),\n",
        "    eliminate_duplicates=True)\n",
        "\n",
        "algorithm1 = NSGA2(\n",
        "    pop_size= 50,\n",
        "    #ref_dirs=get_reference_directions(\"das-dennis\", 3, n_partitions=12),\n",
        "    # sampling=get_sampling(\"int_random\"),\n",
        "    sampling=LHS(),\n",
        "    selection = RandomSelection(),\n",
        "    # crossover=get_crossover(\"int_sbx\", prob=0.9, eta=15),\n",
        "    crossover = SBX(prob=0.6, prob_var=0.5),\n",
        "    mutation=PolynomialMutation(prob=0.5),\n",
        "    eliminate_duplicates=True)\n",
        "\n",
        "# create an instance of termination criterion\n",
        "# termination = get_termination(\"n_gen\", 50)\n",
        "\n",
        "# early stop\n",
        "termination = DefaultMultiObjectiveTermination(\n",
        "    xtol=1e-8,           # movement in the design space xtol\n",
        "    cvtol=1e-6,          # the convergence in the constraint cv_tol\n",
        "    ftol=0.0025,         # objective space f_tol.\n",
        "    period=30,\n",
        "    n_max_gen=50,        # maximum number of generations n_max_gen\n",
        "    n_max_evals=100000   # function evaluations n_max_evals\n",
        ")\n",
        "\n",
        "# perform the optimization\n",
        "res = minimize(problem,\n",
        "               algorithm,\n",
        "               termination,\n",
        "               seed=42,\n",
        "               save_history=True,\n",
        "               verbose=True)\n",
        "res1 = minimize(problem1,\n",
        "               algorithm1,\n",
        "               termination,\n",
        "               seed=42,\n",
        "               seed_population=res.pop,\n",
        "               save_history=True,\n",
        "               verbose=True)\n",
        "\n",
        "# print the results\n",
        "print(f\"Best solution found: \\nX = {res1.X.astype(int)} \\nF = {-res1.F}\") # negate F because we maximized\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the results\n",
        "print(f\"Best solution found: \\nX = {res1.X.astype(int)} \\nF = {-res1.F}\")"
      ],
      "metadata": {
        "id": "NlFRxFda7ICf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241ad6cb-fa04-4548-fbff-0f3203f24a84"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best solution found: \n",
            "X = [[15  4 34  6  0]\n",
            " [15  4 35  6  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 39  6  0]\n",
            " [15  4 36  5  0]\n",
            " [15  4 33  5  0]\n",
            " [15  4 34  6  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  6  0]\n",
            " [15  4 33  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 34  5  0]\n",
            " [15  4 38  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 33  5  0]\n",
            " [15  4 35  6  0]\n",
            " [15  4 34  6  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 34  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 37  6  0]\n",
            " [15  4 35  6  0]\n",
            " [15  4 34  6  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 34  6  0]\n",
            " [15  4 33  5  0]\n",
            " [15  4 36  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 36  6  0]\n",
            " [15  4 33  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  6  0]\n",
            " [15  4 36  5  0]\n",
            " [15  4 34  5  0]\n",
            " [15  4 33  5  0]\n",
            " [15  4 33  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 36  5  0]\n",
            " [15  4 34  6  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 35  5  0]\n",
            " [15  4 37  6  0]\n",
            " [15  4 35  5  0]] \n",
            "F = [[0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]\n",
            " [0.82462687 0.76616915]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}